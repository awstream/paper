\section{\sysname{}}
\label{sec:system}

\sysname{} addresses the application-specific adaptation by separating data
processing from adaptation operations with \texttt{maybe} APIs. Developers
provide hints on potential operations that trade data fidelity with bandwidth
demand without being exact on the numbers. Instead, \sysname{} learns the
concrete degradation strategies using profiling data (either offline or online)
and controls the application execution adaptively. \autoref{fig:overview}
provides an overview of the systems and we will then describe each stage in
detail.

\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{figures/system.pdf}
  \caption{\sysname{} Overview.}
  \label{fig:overview}
\end{figure}

\subsection{Structured Adaptation}
\label{sec:struct-adapt}

\begin{table*}
  \centering
  \begin{tabular}{ c r l }
    \toprule
    \multirow{7}{*}{Normal Operators}
    & \textit{map} (f: I $\Rightarrow$ O) & Stream<I> $\Rightarrow$ Stream<O> \\
    & \textit{filter} (f: I $\Rightarrow$ bool) & Stream<I> $\Rightarrow$
                                                 Stream<I> \\
    & \textit{skip} n(i: Integer) & Stream<I> $\Rightarrow$ Stream<I> \\
    & \textit{sliding\_window} (count: Integer, f: Vec<I> $\Rightarrow$ O) & Stream<I> $\Rightarrow$
                                                                            Stream<O> \\
    & \textit{tumbling\_window} (count: Integer, f: Vec<I> $\Rightarrow$ O) & Stream<I> $\Rightarrow$
                                                                             Stream<O> \\
    & \textit{timed\_window} (time: Duration, f: Vec<I> $\Rightarrow$ O) & Stream<I> $\Rightarrow$
                                                                          Stream<O> \\
    & ... & ... \\
    \midrule
    \multirow{4}{*}{Degradation Operators}
    & \textit{maybe} (knobs: Vec<T>, f:  (T, I) $\Rightarrow$ I) & Stream<I> $\Rightarrow$
                                                                 Stream<I> \\
    & \textit{maybe\_skip} (knobs: Vec<Integer>) & Stream<I> $\Rightarrow$ Stream<I> \\
    & \textit{maybe\_downsample} (knobs: Vec< (Integer, Interger)>) & Stream<Image> $\Rightarrow$ Stream<Image> \\
    & ... & ... \\
    \bottomrule
  \end{tabular}
  \caption{A comparison between normal stream processing operators and our
    degradation operators. \texttt{Vec<T>} represents a list of elements of type
    T. \texttt{Option<T>} indicates an optional element of type T which is
    either present \texttt{Some(T)} or absent \texttt{None}.}
  \label{tab:operators}
\end{table*}

We first consider a strawman solution: manual policies for
degradation. JetStream~\cite{rabkin2014aggregation} offers an example: \textit{``if
bandwidth is insufficient, switch to sending images at 75\% fidelity, then 50\%
if there still isn't enough bandwidth. Beyond that point, reduce the frame rate,
but keep the images at 50\% fidelity.''} We identify the following issues with
this approach.

\para{Lack of precision:} These policies are often developer heuristics and
rarely backed up by measurements. First, there is no direct association of the
application accuracy with the 75\% fidelity configuration. Besides, it's unclear
about how much the policy would affect the data size. For example, one might
think that reducing the frame rate by 50\% will half the data rate. However if
video encoding is employed, when the frame rate reduces, the inter-frame
difference will increased, hence P-frame size. This leads to a larger data size
for each frame.

%% \autoref{fig:h264} illustrates this complex relationship with an example of
%% H.264 encoding under four different frame rates.

\para{Lack of scalablility:} The strawman solution quickly leads to too many
policies when multiple degradation operations are involved or a fine-grained
control is desired. This manual process becomes tedious and error-prone. When
too few rules are provided, the application may oscillate between two rules: one
that's too aggressive (always faces insufficient bandwidth) and one that's too
conservative.

\para{Fixed:} Implemented as part of the application processing, it's fixed and
cannot be updated easily.

When using the above strawman solution, developers are forced to manually study
and measure the impact of individual degradation policy, prohibiting its wide
adoption in practice.

On the other extreme of the design spectrum, a completely developer-free
solution is not practical. While static analysis has been shown to optimize
application execution adaptively in a certain context~\cite{chun2011clonecloud},
they do not work well in our dataflow programming model. Static analysis is
prone to false positives: exploring wrong or unnecessary parameters. For
example, when the application is configured to generates statistics with a
\texttt{timed\_window} operation, static analysis may falsely detect the
duration parameter and alter the behavior of the application in an unexpected
way. Besides, with each introduced parameter, the profiling time increases
drastically as all parameters pose a combinatorial space.

Our system take a middle ground between these two extremes: developers use a
novel \texttt{maybe} API to annotate degradation operations without being exact
on the values. Think of these APIs as hints from developers: this operation,
when in use, will likely reduce the data size and affect the data fidelity;
however the exact impact is not clear.

Applications in \sysname{} are built with a \textit{continuous operator} model,
where computations are constructed with operators and they form a directed
graphs. \sysname{} implements many normal operators that are similar to existing
stream processing systems, such as \texttt{map}, \texttt{filter},
\texttt{sliding\_window} (see \autoref{tab:operators}).

\texttt{maybe} operators to express the specification of degradation. Our
propose APIs do not require developers to be exact on the quantity; integrating
this into existing applications requires minimal effort.

The basic form of \texttt{maybe} operator takes two arguments: a knob and a
degradation function (see \autoref{tab:operators}). The knob indicates different
degradation levels; the function performs the actual degradation operation with
a configurable level. We restrict the type signature of the function that this
API can accept: $f(T, I) \Rightarrow I$. That is, the degradation function
should not alter the type of the stream. While this might seem a strong
restriction, when combined with \texttt{map} operator, the system is still
expressive enough. We describe our implementation and usage
in~\autoref{sec:implementation}.

Based on the \texttt{maybe} primitive, one can implement wrappers for common
degradation operations. For example, \texttt{maybe\_skip} will optionally
subsample a stream; and \texttt{maybe\_downsample} can adjust the image
resolution to a configured target. With this API, the example mentioned earlier
can now be implemented as follows:

\begin{lstlisting}
   let app = Camera::new((1920, 1080, 30))
      .maybe_downsample(vec![(1600, 900), (1280, 720)])
      .maybe_skip(vec![2, 5])
      .map(|frame| frame.show())
      .compose();
\end{lstlisting}

This snippet first instantiate a \texttt{Camera} source, which produces
\texttt{Stream<Image>} with 1920x1080 resolution and 30 FPS. Two degradation
operations are chained after the source: one that downsample the resolution to
either 1600x900 or 1280x720; the other skip the frame with a parameter of 2 or
5, resulting in 30/(2+1)=10 FPS or 30/(5+1)= 6 FPS. After the degradation,
images are shown on the display. In practice, further processing operators can
be chained.

While the API itself has simplified the specification of degradation, the exact
amount has to be known for precise rate adjustment at runtime. We then turn to
the second stage of our system that performs automatic profiling.

\subsection{Automatic Profiling}
\label{sec:automatic-profiling}

The goal of our profiling stage is to explore the bandwidth-accuracy trade-off
and generate a \textit{profile} that is Pareto-optimal.

\subsubsection{Profiling Formalism}
\label{sec:formalize-profiling}

We first define terms and notations we will use. Each \texttt{maybe} operator
within an application corresponds to a knob $k$. Suppose the application has $n$
knobs, their combination forms a configuration $c = [k_{1}, k_{2},
... k_{n}]$. The set of all configurations $\mathbb{C}$ is the space that our
profiling system need to explore.

There are two mappings that we are particularly interested: a mapping from a
particular configuration to its bandwidth requirement $B(c)$ and the accuracy
measure $A(c)$. The Pareto-optimal set $\mathbb{P}$ can then be defined
(\autoref{eq:pareto}): for all $c \in \mathbb{P}$, there is no alternative
configuration $c'$ that requires less bandwidth while giving a higher accuracy.

{\small
\begin{equation}
  \mathbb{P} = \{ c \in \mathbb{C} : \{ c' \in \mathbb{C}: B(c') < B(c),
  A(c') > A(c) \} = \varnothing\}
  \label{eq:pareto}
\end{equation}
}%

\begin{table}
  \centering
  \begin{tabular}{r l}
    \toprule
    \textbf{Symbol} & \textbf{Description} \\
    \midrule
    $n$ & number of degradation operations \\
    $k_i$ & the \textit{i}-th degradation knob \\
    $c = [k_{1}, k_{2}, ... k_{n}]$ & one specific configuration \\
    $\mathbb{C}$ & the set of all configurations \\
    \midrule
    $B(c)$ & bandwidth requirement for $c$ \\
    $A(c)$ & accuracy measure for $c$ \\
    $\mathbb{P}$ & Pareto efficienct set \\
    \bottomrule
  \end{tabular}
  \caption{Notations used in profiling.}
  \label{tab:notations}
\end{table}

Since there is often no closed form relation for $B(c)$ and $A(c)$, our system
takes a data-driven approach: with a representative dataset and an
application-specific utility function, our system evaluates each configuration
for their bandwidth demand and accuracy degradation. The degradation could
either be measured against the groundtruth; or in the case when labelled dataset
is not available, the system uses the reference results when all degradations
are turned off.

\subsubsection{Offline Profiling}
\label{sec:offline-profiling}

To perform an automatic profiling, users provide a representative training data
set and an application-specific accuracy functions.

\para{Profiling is costly; but ok for offline.}

\subsubsection{Online Profiling}
\label{sec:online-profiling}

The model generated offline is susceptible to model drift. Provide an example
here. Hence online profiling is needed.

When doing online profiling, we face two challenges: 1, lack of original data;
2, efficient profiling.

\para{Lack of groundtruth data or reference data.} During the online execution,
it's often not feasible to get groundtruth labelled data. Even with the
undegraded data may not be available.

Allocate some bandwidth for portions of undegraded data. Although the first
scheme seems to be incurring unnecessary bandwidth consumption, when the runtime
is probing for bandwidth, the data can enjoy a free ride.

\para{Efficient profiling.} When doing profiling online, exhaustive exploring
all configurations is costly.

First, we could parallelize the profiling tasks. Better yet, degradation-aware
parallel profiling. Normal job schedulers don't assume the knowledge of
estimated task completion time, therefore the parallel execution suffers from
sub-optimal assignments. Our degradation-aware parallel profiling uses the
execution time information from the offline learned profile to assist
scheduling. The profiling tasks are placed in a queue and all workers follow the
forkâ€“join model of parallel computation.

Second, smaller chunk of data.

Third, trigger-based profiling. To avoid continuous profiling that requires
extensive CPU/GPU resources, our system only triggers profiling when it observed
significant difference between probe configurations and current
configurations. In our scheme, users can either configure the number of probe
configurations, or configure the available resources (CPU-time) and our system
will use offline profiles to calculate the corresponding trigger configurations.

\input{runtime}

\input{multitask}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "sosp17"
%%% End:
