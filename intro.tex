\section{Introduction}
\label{sec:introduction}

An increasing number of streaming data are generated at the edge of the
Internet. These include the growing number of Internet-of-Thing (IoT) devices
and geo-distributed clusters from large companies. Take the video surveillance
as an example, large cities such as New York, Beijing and Seattle are deploying
millions of cameras for traffic control. Retails stores and critical areas such
as railway stations are also employing cameras for abnormally
detection. Traditional view of sensors in IoT are slow-changing environment
monitoring sensors such as temperature or humidity. But the trend has shown an
increasing amount of high-frequency sensor data. BTrDB~\cite{andersen2016btrdb}
handles uPMU monitoring, which is a network of 1000 devices and each produces 12
streams of 120 Hz high-precision values with timestamps accurate to 100 ns. In
total, it accounts to 1.4 million data points per second. Large organizations
are managing 10--100s of datacenters (DCs) and edge clusters
worldwide~\cite{calder2013mapping}. Enormous amount of machine logs are
generated for real-time processing.

Recent stream processing systems that can handle ``big data'', such as
Borealis~\cite{abadi2005design}, Storm~\cite{toshniwal2014storm}, or Spark
Streaming~\cite{zaharia2012discretized}, often focus in the context of a single
cluster, where the bandwidth resources are sufficient (or at least easier to
provision). When processing streams in wide-area, the bandwidth easily becomes
the bottleneck~\cite{rabkin2014aggregation}. It's scarce and has a large
variation over time.

When the network resource becomes oversubscribed (supply fails to meet the
demand), decisions about data communication needs to be
made. Application-agnostic behaviors, such as drop data (UDP) or create
backlogged data (TCP).
% Any application using arial requires running over every frame, drop is bad
% Handling backpressure by retransmission, not good for latency
We demonstrate that either option would lead to significant application
degradation in some cases. In fact, there is not an optimal universal
degradation strategy for all stream processing application. While application
specific solutions can be built in an ad-hoc fashion, our goal is to design APIs
that systematically generates degradation strategies that allows an explicit
exploration of the design space.

The degradation can also be arbitrary user-defined function (UDF) that embeds
deeper knowledge about the particular application. Consider a surveillance
scenario, under normal operation the full scene is transmitted. When the network
condition degrades, the application may choose a critical region to monitor
instead of either dropping frames or reducing the resolution.

% In fact, such APIs can be more powerful than simply hinting at degradation
% strategy. Almost many real-world applications have parameters that controls
% the performance and accurcy of a particular algorithm. When using
% HOG~\cite{dalal2005histograms} for human detection, developers can choose the
% stride window size that controls the number of sliding windows that will be
% applied to the image, therefore controlling processing time as well as
% accuracy.

Our approach combines offline and online profiling to assist the decision
making. The offline process generates a profile for each application that
represents the parameter (knob) space of this application. During the execution,
the online monitoring part will change the application execution. This allows us
to explore and fine-tune the trade-off between application accuracy and system
performance.

% When deploying wide-area analytics applications, we envision it's common that
% multiple applications, which may or may not be developed from the same party,
% will run on the same worker machine. When the resources (CPU, memory and
% bandwidth) are scarce, we need to properly allocate the resource to each
% running application. Combined with our knowledge of the profiling information,
% the the system aims to maximize the overall performance (rather than min-max
% fairness).

We make three contributions in this paper.

\squishlist    %% \begin{itemize}
\item We study how different adaptive schemes impact wide-area streaming
  applications.
\item We propose the combination of offline profiling and online refining
  architecture.
\item We evaluate the system using real-world data and applications.
\squishend %% \end{itemize}

The paper is structured as follows.

\newpage

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "sigcomm2017"
%%% End:
