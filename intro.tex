\section{Introduction}
\label{sec:introduction}

Wide-area streaming analytics are becoming pervasive, especially with the
emerging Internet-of-Thing (IoT) applications. Large cities such as New York,
Beijing and Seattle are deploying millions of cameras for traffic
control~\cite{london.surveillance,skynet}. Retails stores and critical areas
such as railway stations are also being monitored for abnormal
activities. Buildings are increasingly equiped with a wide variety of sensors to
improve building energy use, occupant comfort, reliability and
maintenance~\cite{krioukov2012building}. Geo-distributed infrastructure, such as
Content Delivery Network (CDN), needs to analyze user requests (machine logs)
and optimize data placement to improve delivery efficiency. In these problems,
the data collected at the edge needs to be transported acoross the wide-area and
analyzed in near real-time.

Existing stream processing for ``big data'', such as
Borealis~\cite{abadi2005design}, Storm~\cite{toshniwal2014storm}, or Spark
Streaming~\cite{zaharia2012discretized}, only work in the context of a single
cluster, where the bandwidth is sufficient (or at least easier to
provision). While they are the perfect backends for analyzing streams once data
arrive, in wide-area, the network, with scarce and variable bandwidth, easily
becomes the bottleneck \cite{rabkin2014aggregation}. What's worse, the growth
rate of wide-area network capacity is not keeping up with the increasing rate of
traffic~\cite{index2013zettabyte}.

When facing situations where the bandwidth is not sufficient, applications
deployed today either choose a conservative setting (e.g.\,only delivering 360p
videos) or leave their fate to the underlying transport layer: (1) in the case
of TCP, the sender will be blocked and data are backlogged, leading to severe
delay; (2) in the case of UDP, uncontrolled packet loss occurs, leading to
application performance drop.

The idea of adaptive streaming and trading accuracy for data freshness is not
new. Multimedia applications~\cite{michalos2012dynamic, schulzrinne1998real} has
been extensively studied in the past. However, their optimization goal is to
improve user experience; resulting in an adaptation strategy with a mismatch
from our target video analytical applications (\autoref{sec:making-case-adapt}).

In this paper, we present the design and implementation of \sysname{}, a stream
processing systems for the wide-area. Our system enables applications making
explicit trade-off between data fidelity and freshness with empirical
performance models.

Our system allows developers construct applications that uses the computing
capabilities at the data source site for pre-processing, which has been
demonstrated effective in reducing the bandwidth demand~\cite{pu2015low,
  viswanathan2016clarinet}. There are two types of pre-procssing operations:
lossless or lossy. While lossless transformations are
helpful~\cite{rabkin2014aggregation}, lossy operations often allow more savings
at the expense of reduced accuracy. In this paper, we primarily study the effect
of lossy transformations, i.e.\,degradation.

In video applications, reducing image resolution, frame rate or changing the
video encoding quality are potential degradation operations that affects data
rate and application accuracy. In log analysis, a local thresholding before
transmitting the data is an example of degradation. We notice that these
operations are often more than binary decision; rather they are tunable
\textit{knobs}.

Although the basic idea behind degradation is intuitive, realizing them in
practice is non-trivial. First, degradation has different impact for different
application and data. Second, each degradation is often more than a binary
decision; they are often parameterized and has different impact. Real-world
application also have multiple knobs to tune. It's not always possible to derive
a closed form of analytical relationship between the degradation and its impact
on bandwidth/accuracy. We elaborate on the challenges in
\autoref{sec:challenges}.

% \begin{lstlisting}
% let mut tcp_conn = TcpStream::connect(...)
% let mut hog = cv::Hog::default();
% let stream = Camera::new((1920, 1080, 30))
%         .maybe_downsample(vec![(1600, 900), (1280, 720)])
%         .maybe_skip(vec![2, 5, 9, 14, 29])
%         .map(|frame| h264enc(frame))
%         .write(move |buf| tcp_conn.write(buf))
%         .map(|buf| h264dec(buf))
%         .map(move |frame| hog.detect(frame))
%         .map(|rects| println!("{:?}", rects))
%         .compose();
% \end{lstlisting}

% let stream = Camera::new((1920, 1080, 30))
%         .maybe_downsample(vec![(1600, 900), (1280, 720)])
%         .maybe_skip((2..11).step_by(2))
%         .map(move |frame| hog.detect(frame))
%         .map(|rects| println!("{:?}", rects))
%         .compose();

To make degradation and adaptation practical, \sysname{} employs a data-driven
empirical-analysis approach. First of all, developers express degradation
operations with \sysname{} APIs. These operations are merely hints rather than
exact rules as we do not assume developers are well-aware of the degradation
impact (\autoref{sec:prog-abs}). Instead, \sysname{} performs an automatic
multi-dimensional profiling that produces the bandwidth-accuracy trade-off
\textit{profile}. Developers do need to provide a representative dataset and a
utility function to measure the degradation impact
(\autoref{sec:profiling}). The profile is used in our runtime system. Aided with
bandwidth estimation and congestion controller, the runtime ensures an adaptive
execution in the case of network variation (\autoref{sec:adaptation}).

To study the effect of degradation, we've built three real-world applications
using \sysname{}: a street surveillance application performing pedestrian
detection, an augmented reality application recognizing everyday objects and a
distributed top-k application analyzing web server access logs. The first two
video streaming applications have three knobs: resolution, frame rate and video
encoding quality. For the distributed top-k application, we have two knobs: N in
the local top-N operation and T as a local threshold.

Our offline profiling stage explores the design space for all three applications
and generates the profile consists of Pareto-effiecient configurations. Our
results show that degradation operations often have different impact and many
optimal configurations are only possible when they are combined.

We also evaluate our system's runtime behavior against baseline with controlled
experiment. The evaluations shows that \sysname{} can gracefully handle network
deteriorate where a bounded latency is achieved without too much application
accuracy drop. Even when the bandwidth is nearly halved, we can still achieve
80\% accuracy with 15 seconds delay where TCP creates backlogged for minutes and
UDP only offers accuracy less than 50\%.

In summary, this paper makes the following contributions:

\squishlist    %% \begin{itemize}
\item We study in depth of wide-area streaming applications in the case of
  network resource variation.
\item We design APIs to allow for exploring bandwidth-accuracy trade-off with
  minimal developer effort.
\item We implement the system to perform application profiling and runtime
  adaptation.
\item We build three real-world applications and evaluate their behavior
  under different scenarios.
\squishend %% \end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "sigcomm2017"
%%% End:
