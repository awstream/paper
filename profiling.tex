\subsection{Automatic Profiling}
\label{sec:automatic-profiling}

With structured adaptation, \sysname{} automatically learns an application
profile from \maybe{} operators. The profile accurately captures the
relationship between application accuracy and bandwidth consumption under
different combinations of data degradation operations. We describe the
formalism, followed by techniques that efficiently perform offline and online
profiling.

\para{Profiling formalism:} Suppose a stream processing application has $n$
\maybe{} operators. Each operator introduces a knob $k_i$ and their combination
forms a \textit{configuration} $c = [k_{1}, k_{2}, ... k_{n}]$. The set of all
possible configurations $\mathbb{C}$ is the space that the profiling
explores. For each configuration $c$, there are two mappings that are of
particular interest: a mapping from $c$ to its bandwidth consumption $B(c)$ and
its accuracy measure $A(c)$. \autoref{tab:notations} summarizes notations used
in this paper.

The Pareto-optimal set $\mathbb{P}$ is defined as \autoref{eq:pareto}: it's the
set of all $c$ such that there is no alternative configuration $c'$ that
requires less bandwidth while giving a higher accuracy.

{\small
  \vspace{-1em}
  \begin{equation}
  \mathbb{P} = \{ c \in \mathbb{C} : \{ c' \in \mathbb{C}: B(c') < B(c),
  A(c') > A(c) \} = \varnothing\}
  \label{eq:pareto}
\end{equation}
}%

\begin{table}
  \footnotesize
  \centering
  \begin{tabular}{r l}
    \toprule
    \textbf{Symbol} & \textbf{Description} \\
    \midrule
    $n$ & number of degradation operations \\
    $k_i$ & the \textit{i}-th degradation knob \\
    $c = [k_{1}, k_{2}, ... k_{n}]$ & one specific configuration \\
    $\mathbb{C}$ & the set of all configurations \\
    \midrule
    $B(c)$ & bandwidth requirement for $c$ \\
    $A(c)$ & accuracy measure for $c$ \\
    $\mathbb{P}$ & Pareto-optimal set \\
    \midrule
    $t(c)$ & offline processing time for $c$ \\
    \bottomrule
  \end{tabular}
  \caption{Notations used in this paper.}
  \label{tab:notations}
\end{table}

\sysname{} targets at using arbitrary functions as the degradation
functions. Instead of assuming a closed-form relation for $B(c)$ and $A(c)$,
\sysname{} takes a data-driven approach: profiling applications with
developer-supplied training data.  We measure $B(c)$ at the point of
transmission. The accuracy $A(c)$ is measured either against the groundtruth, or
the reference results when all degradation operations are off.  We discuss
examples of concrete knobs, configurations, $B(c)$ and $A(c)$ when we present
applications in \autoref{sec:build-appl}.

\para{Offline Profiling:} We first use an offline process to build a bootstrap
profile (or default profile). While all knobs form a combinatorial space,
\sysname{} currently performs an exhaustive profiling because this is a one-time
offline process. Future work would explore statistical methods to build
performance models with only a few training
samples~\cite{venkataraman2016ernest}. \sysname{} exploits parallelism within
the configurations profiling task. Without any \textit{a prior} knowledge, all
configurations are assigned randomly to all available machines.

\para{Online Profiling:} \sysname{} runs an online profiling process
continuously to refine the profile. The refinement handles concept drifts, a
problem when the learned profile fails to predict the actual performance
accurately. Online profiling has two challenges:

The first is the lack of groundtruth labels or reference data to compute
accuracy. It's infeasible to label data at runtime as labelling is labor
intensive and time consuming~\cite{russell2008labelme}. One could use raw data
as the reference, but the runtime may be transmitting degraded data due to
bandwidth limits.

\sysname{} currently uses raw data as the reference and allocate additional
bandwidth to back-haul raw data. Allocating additional bandwidth seems a waste
of resources, in our runtime design (see details in \autoref{sec:runtime}), the
raw data can enjoy a free ride during the bandwidth probing.

The second challenge is the efficiency of online profiling. When taking too much
time, the newly-learned profile may already be stale. \sysname{} uses a
combination of parallelism and sampling techniques to speed up this process.

(i) Degradation-aware parallelization: Evaluating each configuration takes a
different amount of time. Typically, an increase in the level of degradation
leads to a decrease in computation, e.g. the smaller the FPS, the fewer images
to process. Therefore, we collect processing time for each configuration from
the offline process and use them for scheduling---such as longest first schedule
(LFS)~\cite{karger2010scheduling}---in parallelization.

(ii) Sampling-based profiling: Instead of profiling \textit{all} the data of
\textit{all} configurations, online profiling can speed up if only done
partially. We can process portions of the raw data; this reduces computation at
the cost of a less accurate profile. Alternatively, we can evaluate a subset of
the configurations and compare the performance with the existing profile. If a
substantial difference is observed, such as more than 1 mbps of bandwidth
estimation, we trigger a full profiling to learn a more accurate profile.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "sosp17"
%%% End:
